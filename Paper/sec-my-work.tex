\section{My work}

\subsection{Idea}

The idea of creating a unified, hyperlinked \cl{} documentation that would additionally span over multiple language extensions and libraries was growing in me since I began my journey with \cl{} back in 2015. I had been irritated by the separation of particular bodies of Lisp knowledge and lack of connection between them. In the beginning of 2016, I started looking for means to improve this situation.

During my research, it became obvious to me that---no matter which particular way would be chosen in this case---the project of creating and maintaining a modern, unified repository of \cl{} documentation would require substantial work. It would be necessary to choose the appropriate pieces of work the repository would consist of, find most recent versions of their documentation, solve any legal issues of creating derivative works of them, parse the existing documents and keep the repository maintained in the face of the changing versions of \cl{} libraries.

\subsection{Requirements} \label{requirements}

The idea for building such a piece of documentation was presented at the European Lisp Symposium 2016 during a lightning talk\cite{els:2016:clus} that I gave. I would like to expand on a particular slide of that presentation, which outlines the qualities I expect of a \cl{} documentation project.
\begin{enumerate}
\item \textbf{Editable:}
It needs to be modifiable and extensible by anyone willing to expand it.
\item \textbf{Complete:}
It should aim for completeness and maximum coverage of the \cl{} universe.
\item \textbf{Downloadable:}
It should be usable locally, without an Internet connection.
\item \textbf{Mirrorable/Clonable:}
It should be easy to create mirrors and copies of it on the Internet and on hard drives.
\item \textbf{Versioned:}
It should use version control.
\item \textbf{Modular:}
It should be splittable into separate modules with cross-module hyperlinks breaking as the only side effect.
\item \textbf{Updatable:}
It should be easy to update it to its newest version.
\item \textbf{Portable:}
It should be exportable as a static HTML website.
\item \textbf{Unified:}
It should be consistent in style.
\item \textbf{Community-based:}
It should belong to the Lisp community and be further developed and extended there.
\end{enumerate}

The implementation of this idea is a project created by me that I have named the \cl{} \us{}, henceforth abbreviated CLUS.

The dpANS\footnote{Draft preview American National Standard.} source\cite{ANSI:1994:draft} makes it \textbf{editable}.

Git\footnote{\url{https://git-scm.com/}} as version control makes it \textbf{downloadable}, \textbf{mirrorable/clonable}, \textbf{versioned} and \textbf{updatable}.

Hosting it on GitHub\footnote{\url{https://github.com}} allows it to be \textbf{community-based}.

DokuWiki\footnote{\url{https://www.dokuwiki.org/}} allows it to be \textbf{modular} and \textbf{portable}.

The goals are---to make it \textbf{complete} and \textbf{unified}.

\subsection{Source}

The whole process was made possible by the availability of the \LaTeX{} source code for ``draft preview Americal National Standard'', abbreviated as dpANS, for \cl{}. These sources were put into public domain by Kent M. Pitman and other members of the X3J13 committee.

While not being the actual standard itself, the dpANS is close enough to it to be usable as a proper reference of \cl{} while also being in the public domain, which allows me to create derivative works of it. It turned out to be a feasible source upon which I could begin implementing the first part of the \us{}.

\subsection{Work done so far}

At the moment of writing these words, I have translated six dictionaries from the dpANS sources into pages in Doku\-Wiki markup syntax, corrected the pages and hyperlinked the code examples found there.

Additionally, I have created a customized version of Doku\-Wiki meant for displaying the CLUS content. While I have not yet published the source code of this modified Doku\-Wiki instance, it was successfully deployed\footnote{\url{http://phoe.tymoon.eu/clus/}} with the specification data translated so far.

I expect to have the whole sources parsed and translated before the European Lisp Symposium 2017.

\subsection{Used methods and tools}

The presence of feasible source for creating a unified and modernized piece of \cl{} documentation allowed me to download the sources and start looking for means of parsing and processing it. The following subchapters describe the tools I have been using and explain the reasons for them being chosen.

\subsubsection{Notepad++---the text editor}

When it came to the main editor for doing most of the parsing work, I could choose between Emacs\footnote{\url{https://www.gnu.org/software/emacs/}} and Note\-pad++\footnote{\url{https://notepad-plus-plus.org/}}, a pair of GPL-licensed\footnote{\url{https://www.gnu.org/licenses/gpl-3.0.en.html}} programmer's editors. Emacs is a keyboard-oriented editor, available for all major operating systems; Notepad++ is a WYSIWYG, keyboard-and-mouse-oriented editor written for Windows that I was able to run on my Linux setup using the Wine\footnote{\url{https://www.winehq.org/}} toolkit.

I ended up doing most of the actual processing of the sources in Notepad++. I found its bulk-editing RegEx features very handy and easy to learn and use. This paper was written in Emacs using its TeX modes.

\subsubsection{DokuWiki---the engine for displaying HTML}

DokuWiki is a GPL-licensed wiki software written in PHP\footnote{\url{http://php.net/}}. In my experience, it was able to fulfill all the requirements I had for a displaying engine. It simply works and allows me to deliver the contents in a readable and aesthetically pleasing way. It does not need database access and instead relies on flat files, which allows me for easy versioning of the data with Git. It has a simple markup syntax that I consider sane. It is extensible and hackable, which so far proves very useful. Also, I have had some previous experience in using and configuring it.

\subsubsection{Regular expressions, GNU coreutils---tools for parsing the sources}

The most important choice that I had to make in the beginning was, how to parse the source files of the dpANS. The source code is a large body of \LaTeX{} code, created by multiple people over a large span of time. It contains highly customized \TeX{} macros, used irregularly among the source code.

The initial research led me towards \TeX{} parsers written in various languages, such as Parsec\footnote{\url{https://wiki.haskell.org/Parsec}} written in Haskell\footnote{\url{https://wiki.haskell.org/}}. My initial attempts of feeding the dpANS sources to the parsers I found were failures though; the individual bodies of code were too complex and my knowledge about these parsers was too little for me to succeed. I realized that, in order to properly parse the \TeX{} source code of the draft, I would need to create a substantially large set of parsing rules; even afterwards, I would need to spend a lot of time doing manual polishing and fixing of the corner cases, such as \TeX{} macros used only in a few places within the source files or actual mistakes within formatting, such as utilizing function markup for macros and vice versa.

Because of this, I decided to abandon the approach of parsing the standard with a parser capable of processing \TeX{} directly and instead go for a simpler choice: utilizing a set of regular expressions to parse a subset of \TeX{} macros and formatting used. It would mean later polishing the preprocessed data by hand, though I would like to note that this last step would be necessary anyway regardless of the technique used.

My editor of choice, Notepad++, contained a powerful enough RegEx engine that was capable of guiding me through the process. Various bulk edits were also made through the assorted Unix utilities: grep, sed, awk, rename.

\subsubsection{Git---versioning system, GitHub---project hosting}

The data for the whole project is kept in a Git repository, stored at GitHub\footnote{\url{https://github.com/phoe/clus-data}} and publicly available. Because DokuWiki keeps all data as flat text files, I can easily modify and deploy new versions of data to upstream websites.

\subsection{Problems encountered}

Most of the problems I have encountered are connected with the dpANS sources being a big and complicated piece of documentation and usage of regular expressions to parse the \TeX{} sources.

As I have mentioned before, the source code had been created over a lengthy period of time with multiple people contributing to it. Because of that, many parts of the specification are formatted differently: they utilize different \TeX{} macros, specific to the people creating the source and the part of the language that was worked upon. Despite the irregularities, I was able to employ the regular expressions and capabilities of my editor to fix most of the cases globally and fix the corner cases manually.

A significant part of the required work was hyperlinking. Although I was able to parse the code for \TeX{} glossary entries, I also needed to take the English grammar into account, such as plural and past forms of glossary entries.

I have had some minor problems with DokuWiki's rendering and markup capabilities, though none of them have been significant enough to be mentioned in detail here.
